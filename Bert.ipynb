{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Collecting pytorch-pretrained-bert\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/a7/278bbec96c9a735049ebf5786fcbeebc8c46ab11cd4472fef77cf2db9fa1/pytorch_pretrained_bert-0.6.2-py2-none-any.whl (106kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 3.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from pytorch-pretrained-bert) (1.0.0)\n",
      "Requirement already satisfied: boto3 in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from pytorch-pretrained-bert) (1.9.82)\n",
      "Collecting regex (from pytorch-pretrained-bert)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/1d/13cc7d174cd2d05808abac3f5fb37433e30c4cd93b152d2a9c09c926d7e8/regex-2019.11.1.tar.gz (669kB)\n",
      "\u001b[K    100% |████████████████████████████████| 675kB 2.6MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from pytorch-pretrained-bert) (2.19.1)\n",
      "Requirement already satisfied: tqdm in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from pytorch-pretrained-bert) (4.26.0)\n",
      "Requirement already satisfied: numpy in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from pytorch-pretrained-bert) (1.15.1)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from boto3->pytorch-pretrained-bert) (0.1.13)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from boto3->pytorch-pretrained-bert) (0.9.3)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.82 in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from boto3->pytorch-pretrained-bert) (1.12.82)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from requests->pytorch-pretrained-bert) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from requests->pytorch-pretrained-bert) (2018.11.29)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from requests->pytorch-pretrained-bert) (2.7)\n",
      "Requirement already satisfied: futures<4.0.0,>=2.2.0; python_version == \"2.6\" or python_version == \"2.7\" in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from s3transfer<0.2.0,>=0.1.10->boto3->pytorch-pretrained-bert) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from botocore<1.13.0,>=1.12.82->boto3->pytorch-pretrained-bert) (2.7.3)\n",
      "Requirement already satisfied: docutils>=0.10 in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from botocore<1.13.0,>=1.12.82->boto3->pytorch-pretrained-bert) (0.14)\n",
      "Requirement already satisfied: six>=1.5 in /Users/shubhammehrotra/anaconda2/lib/python2.7/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.82->boto3->pytorch-pretrained-bert) (1.11.0)\n",
      "Building wheels for collected packages: regex\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/shubhammehrotra/Library/Caches/pip/wheels/5c/c6/c1/0bc8d16ea38c44536a82dd1bec665996e5af37489fa88826b6\n",
      "Successfully built regex\n",
      "Installing collected packages: regex, pytorch-pretrained-bert\n",
      "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.11.1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [00:57<00:00, 7091623.36B/s] \n",
      "100%|██████████| 231508/231508 [00:00<00:00, 3621892.64B/s]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data = ['In what country is Normandy located?', 'Today is a good day']\n",
    "sentences = [\"[CLS] \" + d + \" [SEP]\" for d in data]\n",
    "print (sentences)\n",
    "tokenized_texts = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "print (tokenized_texts)\n",
    "\n",
    "MAX_LEN = 9\n",
    "indexed_tokens = [tokenizer.convert_tokens_to_ids(tokenized_text) for tokenized_text in tokenized_texts]\n",
    "indexed_tokens\n",
    "input_ids = pad_sequences(indexed_tokens, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data in csv\n",
    "dev_set_csv = 'SQuAD-v1.1.csv'\n",
    "\n",
    "# read csv into pandas dataframe\n",
    "data_csv = pd.read_csv(dev_set_csv, encoding = \"ISO-8859-1\")\n",
    "\n",
    "# build dict\n",
    "word2idx = {}\n",
    "idx = 0\n",
    "# for i in range(0, 1):\n",
    "for i in range(0, len(data_csv)):\n",
    "    # add context vocab to dict\n",
    "    context = data_csv['Context'][i]\n",
    "    lst_words_context = re.findall(r\"[\\w']+|[.,!?;]\", context)\n",
    "    for word in lst_words_context:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    # add question vocab to dict \n",
    "    # loop across number of questions for a give context\n",
    "    for j in range(len(data_csv['QuestionAnswerSets'][i].split(\"|\\\"Question\\\"\")[1:])):\n",
    "        question = data_csv['QuestionAnswerSets'][i].split(\"|\\\"Question\\\"\")[1:][j].split(\",\")[0][5:-2]\n",
    "        lst_words_question = re.findall(r\"[\\w']+|[.,!?;]\", question)\n",
    "        for word in lst_words_question:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = idx\n",
    "                idx += 1\n",
    "                \n",
    "\n",
    "# function to return vector of int for str\n",
    "def vec_int(str_ip):\n",
    "    lst_ret = []\n",
    "    lst_str_ip = re.findall(r\"[\\w']+|[.,!?;]\", str_ip)\n",
    "    for word in lst_str_ip:\n",
    "        idx = word2idx[word]\n",
    "        lst_ret.append(idx)\n",
    "        \n",
    "    return lst_ret\n",
    "\n",
    "\n",
    "\n",
    "# create dataframe for getitem\n",
    "df_format = pd.DataFrame(columns = ['Question', 'Context', 'Answer'])\n",
    "for i in range(0, 1000):\n",
    "# for i in range(0, len(data_csv)):\n",
    "    context = data_csv['Context'][i]\n",
    "    for j in range(0, len(data_csv['QuestionAnswerSets'][i].split(\"|\\\"Question\\\"\")[1:])):\n",
    "        question = data_csv['QuestionAnswerSets'][i].split(\"|\\\"Question\\\"\")[1:][j].split(\"->\")[1][2:-14]\n",
    "        \n",
    "        # get start and end indices for answer\n",
    "        s_idx = str(int(data_csv['QuestionAnswerSets'][i].split(\"|\\\"Question\\\"\")[1:][j].split(\"->\")[3][2:-16]) - 1)\n",
    "        e_idx = str(int(s_idx) + len(data_csv['QuestionAnswerSets'][i].split(\"|\\\"Question\\\"\")[1:][j].split(\"->\")[2][3:-22]))\n",
    "        \n",
    "        df_format = df_format.append({'Question' : question , 'Context' : context, 'Answer': (s_idx, e_idx)} , ignore_index = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getQuestionBertEmbeddings(sentence):\n",
    "    sentences = \"[CLS]\" + sentence + \"[SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "    indexed_token = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_token])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    print (segments_tensors, tokens_tensor, len(tokenized_text))\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "    sentence_embedding = torch.mean(encoded_layers[11], 1)\n",
    "    return sentence_embedding\n",
    "\n",
    "def getContextBertEmbeddings(sentence):\n",
    "    sentences = \"[CLS]\" + sentence + \"[SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(sentence)\n",
    "    indexed_token = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    batch_i = 0\n",
    "    print (indexed_token)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_token])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    print (segments_tensors, tokens_tensor, len(tokenized_text))\n",
    "    with torch.no_grad():\n",
    "        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "    token_embeddings = [] \n",
    "\n",
    "# For each token in the sentence...\n",
    "    for token_i in range(len(tokenized_text)):\n",
    "      # Holds 12 layers of hidden states for each token \n",
    "      hidden_layers = [] \n",
    "      # For each of the 12 layers...\n",
    "      for layer_i in range(len(encoded_layers)):\n",
    "        # Lookup the vector for `token_i` in `layer_i`\n",
    "        vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "        hidden_layers.append(vec)\n",
    "      token_embeddings.append(hidden_layers)\n",
    "\n",
    "    # Sanity check the dimensions:\n",
    "    print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
    "    print (\"Number of layers per token:\", len(token_embeddings[0]))\n",
    "\n",
    "    concatenated_last_4_layers = [torch.cat((layer[-1], layer[-2], layer[-3], layer[-4]), 0) for layer in token_embeddings] # [number_of_tokens, 3072]\n",
    "    summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings] # [number_of_tokens, 768]\n",
    "    print (torch.stack(summed_last_4_layers).shape)\n",
    "    return torch.stack(summed_last_4_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        super(dataset, self).__init__()\n",
    "        self.df = data_dir\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i): # return single data item\n",
    "\n",
    "        answerWindow = [int(df_format['Answer'][i][0]), int(df_format['Answer'][i][1])]\n",
    "        bertQuestion = getQuestionBertEmbeddings(df_format['Question'][i])\n",
    "        bertContext = getContextBertEmbeddings(df_format['Context'][i])\n",
    "        print ('Question Shape', bertQuestion.shape)\n",
    "        print ('Context Shape', bertContext.shape)\n",
    "        return torch.LongTensor(vec_int(df_format['Question'][i])), torch.LongTensor(vec_int(df_format['Context'][i])), torch.LongTensor(answerWindow)\n",
    "    \n",
    "train_data = dataset(df_format)\n",
    "# test_data = dataset(df_format)\n",
    "\n",
    "\n",
    "# create train and test dataloader objects\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = 1, shuffle = True) \n",
    "#test_loader = torch.utils.data.DataLoader(test_data, batch_size = bs, collate_fn = collate, shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) tensor([[2054, 2001, 1996, 2516, 1997, 1996, 4516, 1996, 4035, 2207]]) 10\n",
      "[25479, 1005, 1055, 2166, 2001, 3139, 1999, 1037, 4035, 2694, 4516, 25479, 1037, 1996, 2308, 2369, 1996, 2189, 1006, 2230, 1007, 1010, 1998, 1999, 1037, 2230, 4516, 11323, 2011, 12262, 8945, 12036, 22153, 1998, 10704, 4013, 29417, 2050, 2005, 3059, 2547, 1012]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) tensor([[25479,  1005,  1055,  2166,  2001,  3139,  1999,  1037,  4035,  2694,\n",
      "          4516, 25479,  1037,  1996,  2308,  2369,  1996,  2189,  1006,  2230,\n",
      "          1007,  1010,  1998,  1999,  1037,  2230,  4516, 11323,  2011, 12262,\n",
      "          8945, 12036, 22153,  1998, 10704,  4013, 29417,  2050,  2005,  3059,\n",
      "          2547,  1012]]) 42\n",
      "Number of tokens in sequence: 42\n",
      "Number of layers per token: 12\n",
      "torch.Size([42, 768])\n",
      "Question Shape torch.Size([1, 768])\n",
      "Context Shape torch.Size([42, 768])\n"
     ]
    }
   ],
   "source": [
    "for index, (df) in enumerate(train_loader):\n",
    "    question = df[0]\n",
    "    context = df[1]\n",
    "    answer = df[2]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
