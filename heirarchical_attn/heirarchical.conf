[train]
seed: 0
model_type: AttentiveMNMTFeatures
patience: 40
max_epochs: 10
eval_freq: 1000
eval_metrics: rouge,bleu,loss
eval_filters: de-bpe
eval_beam: 4
eval_batch_size: 32
save_best_metrics: True
n_checkpoints: 0
l2_reg: 0
gclip: 1
optimizer: adam
lr: 0.0004
batch_size: 32
save_path: /home/nishad/heirarchical/nmt_exp/models_train
tensorboard_dir: ${save_path}/tb_dir

[model]
att_type: mlp
att_bottleneck: hid
emb_dim: 128
enc_dim: 256
dec_dim: 256
n_encoders: 2
dropout_emb: 0.3
dropout_ctx: 0.3
dropout_out: 0.3
tied_emb: 2way
dec_init: mean_ctx
bucket_by: subs
max_len: 2
fusion_type: hierarchical
direction: para:Text, ques:Numpy -> span:Text
feat_dim: 768

[data]
root: /home/nishad/heirarchical/618

train_set: {'para': '${root}/para_train.txt',
            'ques': '${root}/ques_train.npy',
            'span': '${root}/span_train.txt'}

val_set: {'para': '${root}/para_val.txt',
            'ques': '${root}/ques_val.npy',
            'span': '${root}/span_val.txt'}

test_set: {'para': '${root}/para_val.txt',
            'ques': '${root}/ques_val.npy',
            'span': '${root}/span_val.txt'}

[vocabulary]
para: ${data:root}/para_combined.vocab.txt
span: ${data:root}/span_combined.vocab.txt